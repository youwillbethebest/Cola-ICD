
## üîß Environment Setup

### 1. Create Conda Environment

```bash
conda create -n attentionicd python=3.10
conda activate attentionicd
```

### 2. Install Dependencies

```bash
# Install PyTorch (adjust according to your CUDA version)
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124

# Install other core dependencies
pip install transformers==4.51.3
pip install pandas==2.2.3
pip install pyarrow==18.1.0
pip install scikit-learn==1.6.1
pip install wandb==0.21.0
pip install sentence-transformers==3.3.1
pip install torch-geometric==2.6.1

# Install PyG related libraries (adjust according to CUDA version)
pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html
```

### 3. Download Pretrained Models

The project uses the following pretrained models that need to be downloaded in advance:

```bash
# Text Encoder (choose one)
# - Clinical-Longformer: https://huggingface.co/yikuan8/Clinical-Longformer
# - SapBERT: https://huggingface.co/cambridgeltl/SapBERT-from-PubMedBERT-fulltext

# Label Encoder
# - Bio_ClinicalBERT: https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT
# - SapBERT: https://huggingface.co/cambridgeltl/SapBERT-from-PubMedBERT-fulltext
```


## üìÅ Data Preparation
### 1. MIMIC Dataset Access
Due to the privacy regulations associated with clinical data, we cannot provide the MIMIC-III dataset directly.
* Please request access to MIMIC-III (v1.4) via [PhysioNet](https://physionet.org/content/mimiciii/).
* Complete the required CITI training and sign the Data Use Agreement.

### 2. Preprocessing
To ensure a fair comparison with baselines, we follow the standard data preprocessing pipeline and splits established by Edin et al. (2023)** (see their [reproducibility study](https://github.com/JoakimEdin/medical-coding-reproducibility)).

### Data Directory Structure

```
data/
‚îú‚îÄ‚îÄ mimiciii_full/                    # MIMIC-III full dataset
‚îÇ   ‚îú‚îÄ‚îÄ MIMICIII_train.feather        # Training set
‚îÇ   ‚îú‚îÄ‚îÄ MIMICIII_val.feather          # Validation set
‚îÇ   ‚îú‚îÄ‚îÄ MIMICIII_test.feather         # Test set
‚îÇ   ‚îî‚îÄ‚îÄ icd9_codes_mimiciii.feather   # ICD codes and descriptions
‚îú‚îÄ‚îÄ mimiciii_50/                      # MIMIC-III Top-50 subset
‚îÇ   ‚îú‚îÄ‚îÄ mimiciii_50_train.feather
‚îÇ   ‚îú‚îÄ‚îÄ mimiciii_50_val.feather
‚îÇ   ‚îú‚îÄ‚îÄ mimiciii_50_test.feather
‚îÇ   ‚îî‚îÄ‚îÄ top50.feather
‚îú‚îÄ‚îÄ icd_synonyms_enhanced_gemini.json # ICD synonyms file (optional)
‚îî‚îÄ‚îÄ icd9_abbreviations_gemini.json    # ICD abbreviations file (optional)
```


## üöÄ Quick Start

### 1. Basic Training

```bash
python main.py \
    --train_file data/mimiciii_full/MIMICIII_train.feather \
    --val_file data/mimiciii_full/MIMICIII_val.feather \
    --test_file data/mimiciii_full/MIMICIII_test.feather \
    --codes_file data/mimiciii_full/icd9_codes_mimiciii.feather \
    --pretrained_model_name SapBERT-from-PubMedBERT-fulltext \
    --label_model_name SapBERT-from-PubMedBERT-fulltext \
    --model_type bert_chunk \
    --chunk_size 256 \
    --batch_size 6 \
    --epochs 20 \
    --lr 2e-5 \
    --warmup_steps 2000 \
    --early_stopping
```

### 2. Training with Contrastive Learning

```bash
python main.py \
    --train_file data/mimiciii_full/MIMICIII_train.feather \
    --val_file data/mimiciii_full/MIMICIII_val.feather \
    --test_file data/mimiciii_full/MIMICIII_test.feather \
    --codes_file data/mimiciii_full/icd9_codes_mimiciii.feather \
    --pretrained_model_name SapBERT-from-PubMedBERT-fulltext \
    --label_model_name SapBERT-from-PubMedBERT-fulltext \
    --model_type bert_chunk \
    --chunk_size 256 \
    --batch_size 6 \
    --epochs 20 \
    --lr 2e-5 \
    --warmup_steps 2000 \
    --early_stopping \
    --use_contrastive \
    --contrastive_loss_weight 0.001 \
    --contrastive_temperature 0.3
```

### 3. Training with Synonym Enhancement

```bash
python main.py \
    --train_file data/mimiciii_full/MIMICIII_train.feather \
    --val_file data/mimiciii_full/MIMICIII_val.feather \
    --test_file data/mimiciii_full/MIMICIII_test.feather \
    --codes_file data/mimiciii_full/icd9_codes_mimiciii.feather \
    --pretrained_model_name SapBERT-from-PubMedBERT-fulltext \
    --label_model_name SapBERT-from-PubMedBERT-fulltext \
    --model_type bert_chunk \
    --chunk_size 256 \
    --batch_size 6 \
    --epochs 20 \
    --term_count 4 \
    --synonyms_file data/icd_synonyms_enhanced_gemini.json \
    --early_stopping
```


## ‚öôÔ∏è Parameter Description

### Data-related Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--train_file` | - | Training set path (.feather) |
| `--val_file` | - | Validation set path (.feather) |
| `--test_file` | - | Test set path (.feather) |
| `--codes_file` | - | ICD code file path (.feather) |
| `--synonyms_file` | - | Synonyms file path (.json, optional) |
| `--abbreviations_file` | - | Abbreviations file path (.json, optional) |

### Model-related Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--pretrained_model_name` | Clinical-Longformer | Pretrained model for text encoder |
| `--label_model_name` | Bio_ClinicalBERT | Pretrained model for label encoder |
| `--model_type` | longformer | Model type: longformer, bert_chunk, bert_chunk_v2 |
| `--chunk_size` | 512 | BERT chunk size |
| `--term_count` | 1 | Number of synonyms per label |
| `--max_length` | 4096 | Maximum text length |

### Training-related Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--batch_size` | 12 | Batch size |
| `--epochs` | 5 | Number of training epochs |
| `--lr` | 2e-5 | Learning rate |
| `--warmup_steps` | 0 | Warmup steps |
| `--weight_decay` | 0.0 | Weight decay |
| `--early_stopping` | False | Whether to enable early stopping |
| `--early_stopping_patience` | 5 | Early stopping patience |
| `--scheduler_type` | cosine | Learning rate scheduler type |

### Contrastive Learning Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--use_contrastive` | False | Whether to enable contrastive learning |
| `--contrastive_loss_weight` | 0.1 | Contrastive learning loss weight |
| `--contrastive_temperature` | 0.1 | Contrastive learning temperature |

### GNN-related Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--use_gnn` | False | Whether to enable GNN |
| `--adj_matrix_mode` | ppmi | Adjacency matrix mode: binary, count, ppmi, hierarchy |

### Other Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--use_wandb` | False | Whether to enable W&B logging |
| `--use_amp` | True | Whether to use mixed precision training |
| `--output_dir` | checkpoints/{timestamp} | Model save directory |
| `--threshold` | 0.5 | Classification threshold |

## üìä Model Evaluation

### Evaluate Trained Models

Set `--epochs 0` and specify `--output_dir` as the saved model directory:

```bash
python main.py \
    --train_file data/mimiciii_full/MIMICIII_train.feather \
    --val_file data/mimiciii_full/MIMICIII_val.feather \
    --test_file data/mimiciii_full/MIMICIII_test.feather \
    --codes_file data/mimiciii_full/icd9_codes_mimiciii.feather \
    --batch_size 2 \
    --epochs 0 \
    --term_count 4 \
    --output_dir checkpoints/your_checkpoint_dir
```

### Evaluation Metrics

The model uses the following evaluation metrics:
- **Precision (Macro/Micro)**: Precision rate
- **Recall (Macro/Micro)**: Recall rate
- **F1 Score (Macro/Micro)**: F1 score
- **AUC (Macro/Micro)**: Area Under ROC Curve
- **Precision@K** (K=5, 8, 10, 15): Top-K precision
- **MAP**: Mean Average Precision
